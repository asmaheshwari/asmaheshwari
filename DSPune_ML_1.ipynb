{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What are the three stages to build the hypotheses or model in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Three stages to build the hypothesis or model in machine learning are\n",
    "1. Select Data <br>\n",
    "2. Preprocess Data <br>\n",
    "3. Transform Data <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What is the standard approach to supervised learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standard appraoch to supervised learning is \n",
    "\n",
    "1. From population select random sample set\n",
    "2. Split the random sample set into \n",
    "    - Training \n",
    "    - Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is Training set and Test set?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training set is a dataset used to train a model.  In training the model, specific features are picked out from the training set.  These features are then incorporated into the model. Thereby, if the training set is labeled correctly, the model should be able to learn something from these features. With Training set we derive predective analysis for the population data.\n",
    "\n",
    "- Test set is a dataset used to measure how well the model performs at making predictions on that test set. If the prediction scores for the test set are unreasonable as compared to hypothesis derived from training, we make some adjustments to our model and try again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the general principle of an ensemble method and what is bagging and boosting in ensemble method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods are supervized learning models which combine the predictions of multiple smaller models to improve predictive power and generalization.\n",
    "- Bagging  -  Several estimators are built separately (Parallely) on subsets of the data and their predictions are averaged. Bagging can reduces variance with some or no effect on bias. <Br> <b> Example </b> : Random Forest\n",
    "- Boosting - Base estimators are built sequentially. Each subsequent estimator focuses on the weaknesses of the previous estimators. In the sense several weak models group up to produce a powerful ensemble model. <Br>  <b>Example</b> : XGBoost, Gradient Boosted Tree, AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. How can you avoid overfitting ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. \n",
    "To avoid overfitting we have few options which can be followed \n",
    "1. Cross-validation : is a powerful preventative measure against overfitting.\n",
    "2. Train with more data : Ensure that data is clean and relevent, if noise data is added then this will not work. \n",
    "3. Remove features : if anything does not make sense and hard to justify use of the feature, it is better to remove them.\n",
    "4. Early Stop : refers stopping the training process before the learner passes that point.\n",
    "5. Regularization : techniques for artificially forcing your model to be simpler by purning a decision tree, adding penalty to cost function in regression. \n",
    "6. Ensembling : are machine learning methods for combining predictions from multiple separate models. There are a few different methods for ensembling, but the two most common are:\n",
    "    - Bagging attempts to reduce the chance overfitting complex models.\n",
    "    - Boosting attempts to improve the predictive flexibility of simple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
